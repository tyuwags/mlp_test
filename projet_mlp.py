# -*- coding: utf-8 -*-
"""Projet_mlp

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e2VXJQ194D8YncyhxaUQr41f_agVF8vl
"""

import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, RobustScaler
import tensorflow as tf
from scipy.fft import fft

!wget https://maxime-devanne.com/datasets/ECG200/ECG200_TRAIN.tsv
!wget https://maxime-devanne.com/datasets/ECG200/ECG200_TEST.tsv

"""Parameter to test the model"""

norm = True
fft = False
augment = True
enlarge = False

@tf.keras.saving.register_keras_serializable()
class FourierTransform1D(tf.keras.layers.Layer):
    def __init__(self, **kwargs):
        super(FourierTransform1D, self).__init__(**kwargs)

    def call(self, inputs):
        inputs_complex = tf.complex(inputs, tf.zeros_like(inputs, dtype=tf.float32))
        fft_result = tf.signal.fft(inputs_complex)
        real_part = tf.math.real(fft_result)
        imag_part = tf.math.imag(fft_result)
        return tf.concat([real_part, imag_part], axis=-1)  # Concatenate real and imaginary parts

    def compute_output_shape(self, input_shape):
        return input_shape[:-1] + (2 * input_shape[-1],)

df = pd.read_csv("ECG200_TRAIN.tsv", delimiter="\t", header=None)
df.head()
df.dropna()

x_train = np.asarray(df.iloc[:, 1:], dtype=np.float32)
y_train = np.asarray(df.iloc[:, :1], dtype=np.float32)

df = pd.read_csv("ECG200_TEST.tsv", delimiter="\t", header=None)
df.head()
df.dropna()
x_test = np.asarray(df.iloc[:, 1:], dtype=np.float32)
y_test = np.asarray(df.iloc[:, :1], dtype=np.float32)

def jitter(data, std_dev=0.1):
      return data + np.random.normal(scale=std_dev, size=data.shape)

def shift_samples(data, shift_steps=None):
    if shift_steps is None:
      shift_steps = np.random.randint(-30, 30)
    shifted_sample = np.roll(data, shift_steps)
    return shifted_sample

def time_warp(data, warp_factor=0.2):
    T = data.shape[0]
    # Generate random time warping factor
    warp_factor = np.random.normal(loc=1.0, scale=warp_factor)
    # Create linearly spaced indices for the warped time axis
    warped_indices = np.linspace(0, T - 1, T) * warp_factor
    # Interpolate the data using the warped indices
    warped_data = np.interp(warped_indices, np.arange(T), data)
    return warped_data

def scale(data, factor_range=(0.8, 1.2)):
    factor = np.random.uniform(*factor_range)
    return data * factor

def augment_sample(sample, augmentation_functions):
    augmented_sample = sample.copy()
    selected_functions = np.random.choice(augmentation_functions, size=np.random.randint(1, len(augmentation_functions) + 1), replace=False)
    for i, augmentation_function in enumerate(selected_functions):
      augmented_sample = augmentation_function(augmented_sample)
    return augmented_sample

def augment_data(x_train, y_train, number):
  augmentation_functions = [jitter, scale, shift_samples]

  for i, sample in enumerate(x_train.copy()):
    for j in range(number-1):
      x_train = np.vstack([x_train, augment_sample(sample, augmentation_functions)])
      y_train = np.vstack([y_train, y_train[i]])

  c = np.c_[x_train.reshape(len(x_train), -1), y_train.reshape(len(y_train), -1)]
  a = c[:, :x_train.size//len(x_train)].reshape(x_train.shape)
  b = c[:, x_train.size//len(x_train):].reshape(y_train.shape)
  np.random.shuffle(c)
  return a,b

def enlarge_timeserie(x_train, number):
  augmented_samples = np.empty((0, x_train.shape[1]*number))
  augmentation_functions = [jitter, scale]
  for sample in x_train:
    augmented_sample = sample
    for _ in range(number - 1):
      augmented_sample = np.concatenate((augmented_sample, augment_sample(sample, augmentation_functions)))
    augmented_samples = np.vstack([augmented_samples, augmented_sample])
  return augmented_samples

def enlarge_timeseries(x_train, number):
  first = x_train
  for _ in range(number-1):
    x_train = np.hstack([x_train, first])
  return x_train

y_train = (y_train == 1).astype(int)
y_test = (y_test == 1).astype(int)

if augment:
  x_train, y_train = augment_data(x_train, y_train, 100)
  print(x_train.shape)

if enlarge:
  x_train = enlarge_timeseries(x_train, 10)
  x_test = enlarge_timeseries(x_test, 10)
  print(x_train.shape)

"""With normalization in case"""

if norm:
  robust_scaler = RobustScaler()
  x_train_robust = robust_scaler.fit_transform(x_train)
  x_test_robust = robust_scaler.transform(x_test)

  scaler = MinMaxScaler()
  x_train = scaler.fit_transform(x_train_robust)
  x_test = scaler.transform(x_test_robust)

print(x_train.shape)
input_shape = x_train.shape[1]
#input_layer = tf.keras.layers.Input(input_shape)
#hidden_layer = input_layer
model_mlp = tf.keras.Sequential()

# the hidden layer is connected to the input layer
model_mlp.add(tf.keras.layers.Flatten(input_shape=(input_shape,)))

if fft:
  #hidden_layer = FourierTransform1D()(hidden_layer)
  model_mlp.add(FourierTransform1D())

for i in range(1):
  model_mlp.add(tf.keras.layers.Dense(units=50, activation="relu"))
  model_mlp.add(tf.keras.layers.BatchNormalization())
  model_mlp.add(tf.keras.layers.Dense(units=150, activation="relu"))
  model_mlp.add(tf.keras.layers.BatchNormalization())
  model_mlp.add(tf.keras.layers.Dense(units=900, activation="relu"))
  model_mlp.add(tf.keras.layers.BatchNormalization())
  model_mlp.add(tf.keras.layers.Dense(units=400, activation="relu"))

model_mlp.add(tf.keras.layers.Dense(1, activation="sigmoid"))
#hidden_layer = tf.keras.layers.Dense(units=input_shape, activation='relu')(hidden_layer)
#hidden_layer = tf.keras.layers.Dense(units=256, activation='relu')(hidden_layer)
#hidden_layer = tf.keras.layers.Dense(units=1024, activation='relu')(hidden_layer)
#hidden_layer = tf.keras.layers.Dense(units=128, activation='relu')(hidden_layer)
#hidden_layer = tf.keras.layers.Dense(units=5, activation='relu')(hidden_layer)
#hidden_layer = tf.keras.layers.Dropout(0.2)(hidden_layer)
#output_layer = tf.keras.layers.Dense(units=1,activation='tanh')(hidden_layer)
#model_mlp = tf.keras.models.Model(inputs=input_layer, outputs=output_layer)

tf.keras.utils.plot_model(model_mlp, show_shapes=True)

# Optimizer : Adam
initial_learning_rate = 0.1
decay_rate = 0.95
decay_steps = 256

learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate=initial_learning_rate,
    decay_rate=decay_rate,
    decay_steps=decay_steps
)
optimizer_algo = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)

#optimizer_algo = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)

# Cost function : Categorical Cross-Entropy
cost_function = tf.keras.losses.BinaryCrossentropy()

# Model compilation with the 'accuracy' as additional metric
model_mlp.compile(loss=cost_function,optimizer=optimizer_algo, metrics=["accuracy"])

nb_epochs = 256
mini_batch_size = 32
percentage_of_train_as_validation = 0.2
model_checkpoint = tf.keras.callbacks.ModelCheckpoint('best-model.keras', monitor='val_loss', save_best_only=True, patience=10, restore_best_weights=True)

history_mlp = model_mlp.fit(x_train, y_train, batch_size=mini_batch_size,
                    epochs=nb_epochs,verbose=True,
                    validation_split=percentage_of_train_as_validation,
                    callbacks=[model_checkpoint])

history_dict = history_mlp.history
loss_train_epochs = history_dict['loss']
loss_val_epochs = history_dict['val_loss']

plt.figure()
plt.plot(loss_train_epochs,color='blue',label='train_loss')
plt.plot(loss_val_epochs,color='red',label='val_loss')
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend()
plt.savefig('epoch-loss.pdf')
plt.show()
plt.close()

acc_train_epochs = history_dict['accuracy']
acc_val_epochs = history_dict['val_accuracy']

plt.figure()
plt.plot(acc_train_epochs,color='blue',label='train_acc')
plt.plot(acc_val_epochs,color='red',label='val_acc')
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.legend()
plt.show()
plt.close()

model = tf.keras.models.load_model('best-model.keras', custom_objects={"FourierTransform1D": FourierTransform1D})

# evaluation on train
loss,acc = model.evaluate(x_train, y_train,verbose=False)
print("Accuracy on train:",acc)

# evaluation on test
loss,acc = model.evaluate(x_test,y_test,verbose=False)
print("Accuracy on test:",acc)